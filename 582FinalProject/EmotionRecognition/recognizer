import sys
import scipy.io.wavfile
import emoji

sys.path.append("../OpenVokaturi-3-0/api")
import Vokaturi
Vokaturi.load("../lib/open/win/OpenVokaturi-3-0-win32.dll")

file_name = input("Name of file please: ")

(sample_rate, samples) = scipy.io.wavfile.read(file_name)

buff_length = len(samples)
c_buffer = Vokaturi.SampleArrayC(buff_length)

if samples.ndim == 1:  # mono
	c_buffer[:] = samples[:] / 32768.0
else:  # stereo
	c_buffer[:] = 0.5*(samples[:,0]+0.0+samples[:,1]) / 32768.0

voice = Vokaturi.Voice (sample_rate, buff_length)
voice.fill(buffer_length, c_buffer)
soundQuality = Vokaturi.Quality()

emoProbs = Vokaturi.EmotionProbabilities()
voice.extract(quality, emotionProbabilities)

if quality.valid:
	print ("Neutral: %.3f" % emotionProbabilities.neutrality)
	print ("Happy: %.3f" % emotionProbabilities.happiness)
	print ("Sad: %.3f" % emotionProbabilities.sadness)
	print ("Angry: %.3f" % emotionProbabilities.anger)
	print ("Fear: %.3f" % emotionProbabilities.fear)
else:
	print ("Not enough sonorancy to determine emotions")

voice.destroy()
